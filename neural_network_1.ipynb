{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ektaarora3501/Neural-Networks/blob/master/neural_network_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxzAp5gs1bzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cG2fqXd16jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "n_inputs = 28*28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "tf.reset_default_graph()\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
        "\n",
        "\n",
        "def neuron_layer(X, n_neurons, name, activation=None):\n",
        "      with tf.name_scope(name):\n",
        "          n_inputs = int(X.get_shape()[1])\n",
        "          stddev = 2 / np.sqrt(n_inputs)\n",
        "          init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
        "          W = tf.Variable(init, name=\"kernel\")\n",
        "          b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
        "          Z = tf.matmul(X, W) + b\n",
        "          if activation is not None:\n",
        "                 return activation(Z) \n",
        "          else:\n",
        "                 return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE708q-AcHCf",
        "colab_type": "text"
      },
      "source": [
        "specifying number of inputs, number of neurons in each hidden layers\n",
        "defining a neuron layer function to calculate the weight every and output value z=w*x+b .Name scope of tensorflow is just to ensure all the nodes are part of same graph for a smooth functioning of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXRDnIk2XAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "with tf.name_scope(\"dnn\"):\n",
        "  hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",activation=tf.nn.relu)\n",
        "  hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",activation=tf.nn.relu)\n",
        "  logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXyPVF6djyZ",
        "colab_type": "text"
      },
      "source": [
        "Delaring node name as dnn\n",
        "Creating layers that will receive input from previous layer and give output accordingly\n",
        "logit is the final layer containing output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmqYlBnE4J0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
        "  loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_uQoP6AennX",
        "colab_type": "text"
      },
      "source": [
        "Calculating error/loss using entropy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRPGL-cE4fXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "with tf.name_scope(\"train\"):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "  correct = tf.nn.in_top_k(logits, y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "  print(correct)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wamEMnEkev6j",
        "colab_type": "text"
      },
      "source": [
        "creating the training model using gradient descent optimizer \n",
        "eval scope converts the logits into one hot encoded matrix.\n",
        "Basically choosing the feature that best fits the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFS9Z8Si442x",
        "colab_type": "code",
        "outputId": "439e950d-7e03-4f75-8225-a4aa97f10486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
        "n_epochs = 40\n",
        "batch_size = 50\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for iteration in range(mnist.train.num_examples//batch_size):\n",
        "        X_batch,y_batch=mnist.train.next_batch(batch_size)\n",
        "        sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
        "    acc_train=accuracy.eval(feed_dict={X:X_batch,y:y_batch})\n",
        "    #acc_test=accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.images})\n",
        "    \n",
        "    print(epoch,\"training_acuuracy :\",acc_train\"test accuracy :\",acc_test)\n",
        "  save_path=saver.save(sess, \"./my_model_final.ckpt\")\n",
        "  print(mnist.test.images)\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "0 training_acuuracy : 0.88\n",
            "1 training_acuuracy : 0.94\n",
            "2 training_acuuracy : 0.9\n",
            "3 training_acuuracy : 0.98\n",
            "4 training_acuuracy : 0.96\n",
            "5 training_acuuracy : 0.96\n",
            "6 training_acuuracy : 0.98\n",
            "7 training_acuuracy : 0.94\n",
            "8 training_acuuracy : 0.96\n",
            "9 training_acuuracy : 0.94\n",
            "10 training_acuuracy : 0.96\n",
            "11 training_acuuracy : 0.98\n",
            "12 training_acuuracy : 1.0\n",
            "13 training_acuuracy : 1.0\n",
            "14 training_acuuracy : 0.94\n",
            "15 training_acuuracy : 0.98\n",
            "16 training_acuuracy : 1.0\n",
            "17 training_acuuracy : 1.0\n",
            "18 training_acuuracy : 0.98\n",
            "19 training_acuuracy : 0.98\n",
            "20 training_acuuracy : 1.0\n",
            "21 training_acuuracy : 1.0\n",
            "22 training_acuuracy : 0.96\n",
            "23 training_acuuracy : 1.0\n",
            "24 training_acuuracy : 1.0\n",
            "25 training_acuuracy : 0.92\n",
            "26 training_acuuracy : 1.0\n",
            "27 training_acuuracy : 1.0\n",
            "28 training_acuuracy : 0.98\n",
            "29 training_acuuracy : 1.0\n",
            "30 training_acuuracy : 0.98\n",
            "31 training_acuuracy : 1.0\n",
            "32 training_acuuracy : 0.98\n",
            "33 training_acuuracy : 0.98\n",
            "34 training_acuuracy : 1.0\n",
            "35 training_acuuracy : 1.0\n",
            "36 training_acuuracy : 0.98\n",
            "37 training_acuuracy : 1.0\n",
            "38 training_acuuracy : 0.98\n",
            "39 training_acuuracy : 1.0\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
